---
output: pdf_document
header-includes:
 - \usepackage{setspace}\doublespacing
fontsize: 12pt
title: "Electric Grid Stability Analysis"
author: "William G. Stalnaker (wgstalnaker)"
date: "June 21, 2020"
---

\pagebreak

\begin{center}
\textbf{Introduction}
\end{center}

Today’s electrical grid, which the majority of businesses and residents connect to, is experiencing a new market disruption. The disruptive technology of distributed energy generation, such as residential solar, has introduced new factors impacting grid stability. The grid stability is not immune to other disruptions, such as aging infrastructure, cyber security, and natural disasters. Each of these disruptions have a direct impact to the supply and demand economics of the electrical grid as a whole.

To reduce the impact, decentralizing the grid has been proposed. Arzamasov, Böhm and Jochem, of the Karlsruhe Institute of Technology in Germany suggest (2018), “Decentralize Smart Grid Control (DSGC) is a new system implementing demand response without significant changes of the infrastructure.” Analysis of the factors and impacts associated with implementing a DSGC system are outside of the scope of this paper. This paper will focus on implementation of machine learning models to predict stability of the grid, utilizing the data set Arzamasov created (2018). This data set is a simulation of a four-node star electrical grid with centralized production. The data set includes 14 total attributes - 11 predictive attributes, one non-predictive (p1), and two goal fields, as Arzamasov details below (2018):

* tau[x]: reaction time of participant (real from the range [0.5,10]s). Tau1 - the value for electricity producer.
* p[x]: nominal power consumed(negative)/produced(positive)(real). For consumers from the range [-0.5,-2]s^-2; p1 = abs(p2 + p3 + p4)
* g[x]: coefficient (gamma) proportional to price elasticity (real from the range [0.05,1]s^-1). g1 - the value for electricity producer.
* stab: the maximal real part of the characteristic equation root (if positive - the system is linearly unstable)(real)
* stabf: the stability label of the system (categorical: stable/unstable)

The Electrical Grid Stability Simulated Data data set was split 80/20. According to the random creation process Vadim Arzamasov described (2018), this allowed for the test set to be large enough to be a satisfactory representation of the data set as a whole and return statistically meaningful results. The caret package was leveraged heavily for modeling. Model algorithms were selected for the tasks of performing classification and regression as well as those referenced in *Towards Concise Models of Grid Stability* (Arzamasov, Böhm, and Jochem, 2018). The selected algorithms were:

* Classification and Regression Trees
* k-Nearest Neighbors
* Naive Bayes
* Random Forest
* Support Vector Machines with Radial Basis Function Kernel

These algorithms were executed using the caret package's train function while applying 10 fold cross validation. The model with the highest accuracy in predicting stabf was selected for making final predictions. Support Vector Machines with Radial Basis Function Kernel, with an accuracy of 95%, will prove to be the best algorithm for predicting grid stability.

\begin{center}
\textbf{Methodology}
\end{center}
```{r, include=FALSE}

#############################################################
# Load Required Packages and download the Electrical Grid 
# Stability Simulated project dataset from UCI Machine 
# Learning Repository
# 
# Abstract: The local stability analysis of the 4-node star system (electricity
# producer is in the center) implementing Decentral Smart Grid Control concept.
#
# Source:
# -- Creator and donor: Vadim Arzamasov (vadim.arzamasov '@' kit.edu),
# Department of computer science, Karlsruhe Institute of Technology; Karlsruhe,
# 76131; Germany -- Date: November, 2018
#
# Data Set Information:
# The analysis is performed for different sets of input values using the
# methodology similar to that described in [SchÃ¤fer, Benjamin, et al. 'Taming
# instabilities in power grid networks by decentralized control.' The European
# Physical Journal Special Topics 225.3 (2016): 569-582.]. Several input values
# are kept the same: averaging time: 2 s; coupling strength: 8 s^-2; damping:
# 0.1 s^-1
#
# Attribute Information:
# 11 predictive attributes, 1 non-predictive(p1), 2 goal fields:
# 1. tau[x]: reaction time of participant (real from the range [0.5,10]s). Tau1
# - the value for electricity producer.
# 2. p[x]: nominal power consumed(negative)/produced(positive)(real). For
# consumers from the range [-0.5,-2]s^-2; p1 = abs(p2 + p3 + p4)
# 3. g[x]: coefficient (gamma) proportional to price elasticity (real from the
# range [0.05,1]s^-1). g1 - the value for electricity producer.
# 4. stab: the maximal real part of the characteristic equation root (if
# positive - the system is linearly unstable)(real)
# 5. stabf: the stability label of the system (categorical: stable/unstable)

#############################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# Electrical Grid Stability Simulated dataset:
# https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv
dl <- tempfile()
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv", dl)

egss <- read.csv2(dl, header = TRUE, sep = ",")
# Set data types to align with source
egss <- as.data.frame(egss) %>% mutate(tau1 = as.numeric(tau1),
                                       tau2 = as.numeric(tau2),
                                       tau3 = as.numeric(tau3),
                                       tau4 = as.numeric(tau4),
                                       p1 = as.numeric(p1),
                                       p2 = as.numeric(p2),
                                       p3 = as.numeric(p3),
                                       p4 = as.numeric(p4),
                                       g1 = as.numeric(g1),
                                       g2 = as.numeric(g2),
                                       g3 = as.numeric(g3),
                                       g4 = as.numeric(g4),
                                       stab = as.numeric(stab),
                                       stabf = as.factor(stabf))
# Create dataframe including the 11 predictors and 1 classification 
egss_df <- as.data.frame(egss[,c(1,2,3,4,6,7,8,9,10,11,12,14)])
# Test set will be 20% of Electrical Grid Stability Simulated data
set.seed(1, sample.kind="Rounding")
# Using caret packages createDataPartition function to split training and test sets
test_index <- createDataPartition(y = egss_df$stabf, times = 1, p = 0.2, list = FALSE)
train <- egss_df[-test_index,]
temp <- egss_df[test_index,]
# Make sure stabf in test set are also in train set
test <- temp %>% 
  semi_join(train, by = "stabf")
# Add rows removed from test set back into train set
removed <- anti_join(temp, test)
train <- rbind(train, removed)
# Remove staging data used to create training and test sets
rm(dl, test_index, temp, removed)

```

### Data Ingestion

The Electrical Grid Stability Simulated data set used for this project is held in the UCI Machine Learning Repository (2019). This copy is stored in a comma delimited format which requires data types to be mutated to usable formats. Attributes 1-13 are converted to numeric, and the 14th attribute is converted to a factor. During data ingestion, the training and testing data sets are created and represent an 80/20 split of the original data set. As previously mentioned, this constitutes a test set large enough to be a satisfactory representation of the data set as a whole and sufficient to return statistically meaningful results. To avoid unanticipated interference during training and testing, non-predictor attributes were removed from the data set.

### Data Structure
 
```{r, echo=FALSE}
str(egss)
```

The data is tidy, and the ingestion script has successfully transformed the data to the correct format for this analysis.

\pagebreak

### Null Values Check

```{r, echo=FALSE}
# Check for null values
sapply(egss, {function(x) any(is.na(x))})
```

No null values to be handled.

### Data Summary

```{r, echo=FALSE}
# Data summary
summary(egss)
```

**Figure 1**

*Illustration of grid stability and power consumption distributions*

```{r, echo=FALSE, out.width='.49\\linewidth', fig.width=3, fig.height=3, fig.show='hold', fig.align='center'}
# Plotting stablitiy
ggplot(egss, aes(stabf)) +
  geom_bar(fill = "blue", col = "black") +
  xlab("Distribution of Grid Stability")

# Consumer (p[x]: nominal power consumed(negative)/produced(positive)(real). For
# consumers from the range [-0.5,-2]s^-2; p1 = abs(p2 + p3 + p4) )
ggplot(egss, aes(p1)) +
  geom_histogram(binwidth = .1, fill = "blue", col = "black") +
  xlab("Distribution of Power Consumption")
```

\pagebreak

This data exploration shows that the data is tidy. There are no null values to be handled. The proportion of unstable is greater than the stable, representing 64% of the entire data set. Finally, there is a normal distribution of the Distribution of Power Consumption of the p1 attribute. The p1 was defined as representation of the consumers and the actual is calculated as p1 = abs(p2 + p3 + p4).

When visualizing the 11 predictors across the stabf goal attribute there is even distribution with very little variance. Could this be an indication that the data set is overtly normal or simply minimal variances in electrical grid supply and demand disruptions?

**Figure 2**

*Illustration of the density of the stabf attribute within p1 and each of the predictor attributes*

```{r, echo=FALSE}
# Distribution of stablitiy across predicters
featurePlot(x = egss[, 1:12], 
            y = egss$stabf, 
            plot = "density",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))
```

\pagebreak

### Modeling

The modeling approach used for this analysis was to leverage the caret package. The previous data exploration and the data set document on the UCI Repository (2019) show the following:

* Data is clean and normalized
* Data includes 10,000 rows of 14 attributes
* Data includes 11 predictors
* Data includes one non-predictive value (p1)
* Data includes two goal attributes (stab and stabf)
* Well-suited for Classification and Regression

*Note: If further exploration is desired, additional information on this package may be found here: https://cran.r-project.org/web/packages/caret/vignettes/caret.html*

Given the data is well-suited for Classification and Regression, these model types will be utilized by leveraging the train function of the caret package. Kuhn (2019) instructs that the train function can be used to:

* evaluate, using resampling, the effect of model tuning parameters on performance
* choose the ``optimal’’ model across these parameters
* estimate model performance from a training set

Training model evaluation will use 10-fold cross validation. Five different models will be evaluated, and the model that produces the greatest accuracy will be chosen to make the final prediction. Where appropriate, that model will be tuned for optimal accuracy.

\pagebreak

```{r, include=TRUE}
# Run algorithms using 10-fold cross validation
fitControl <- trainControl(method="cv", number=10)
```

```{r, include=TRUE}
# Goal is to find the most accurate model, the metric parameter
# will be used for this.
metric <- "Accuracy"
# Classification and Regression Trees (CART)
set.seed(825)
fit_cart <- train(stabf~., data=train, 
                  method="rpart", 
                  metric=metric, 
                  trControl=fitControl)
# Classification and Regression Trees (CART) - Results
# fit_cart$bestTune
# k-Nearest Neighbors (kNN).
set.seed(825)
fit_knn <- train(stabf~., data=train, 
                 method="knn", 
                 metric=metric, 
                 tuneGrid = data.frame(k = seq(2 , 50 , 2)),
                 trControl=fitControl)
# k-Nearest Neighbors (kNN) results
#fit_knn$bestTune
#plot(fit_knn)
#fit_knn$finalModel
# Naive Bayes (naive_bayes)
set.seed(825)
fit_nb <- train(stabf~., data=train, 
                 method="naive_bayes", 
                 metric=metric, 
                 trControl=fitControl)
# Naive Bayes (naive_bayes) - Results
#fit_nb$finalModel
#  Random Forest (RF)
set.seed(825)
fit_rf <- train(stabf~., data=train, 
                method="rf", 
                metric=metric, 
                tuneGrid = expand.grid(.mtry=c(1:5)),
                trControl=fitControl)
# Support Vector Machines (SVM) with a linear kernel
set.seed(825)
fit_svm <- train(stabf~., data=train, 
                 method ="svmRadial", 
                 metric = metric, 
                 tuneGrid = expand.grid(sigma = c(.01, .50, .05),
                                        C = c(0.75, 0.9, 1, 1.1, 1.25)),
                 trControl = fitControl,
                 preProcess = c("center","scale"))
# Support Vector Machines (SVM) with a linear kernel - Results
#fit_svm$bestTune
#plot(fit_svm)
#fit_svm$finalModel
# summarize accuracy of models
results <- resamples(list(cart=fit_cart,
                          nb=fit_nb,
                          knn=fit_knn, 
                          rf=fit_rf,
                          svm=fit_svm))

```

\begin{center}
\textbf{Results}
\end{center}

The accuracy of the five models is shown in the following plot. Support Vector Machines with Radial Basis Function Kernel achieved the highest accuracy on the training data set and will be used for the final prediction using the test data set.

**Figure 3**

*Dot plot illustrating the accuracy of the five different models*

```{r, echo=FALSE}
# compare accuracy of models
dotplot(results)
```

Additionally, we can use the following plot to visualize the optimal sigma tuning parameter:

  **Figure 4**

*Illustration of the tuning parameters of the Support Vector Machines (SVM)*

```{r, echo=FALSE}
# Support Vector Machines (SVM) with a linear kernel - Results
plot(fit_svm)
```


```{r, include=TRUE}
# Support Vector Machines (SVM) with a linear kernel
# Results as shown in the stored finalModel
fit_svm$finalModel
```

### Final Model
```{r message=FALSE, warning=FALSE, include=TRUE}
# Support Vector Machines (SVM) with a linear kernel on the test dataset
predictions <- predict(fit_svm, test)
cm <- confusionMatrix(predictions, test$stabf)
print(cm)
```

\begin{center}
\textbf{Conclusion}
\end{center}

The following confusion matrix shared on www.stackoverflow.com (2017) summarizes the outcome of the Support Vector Machines with Radial Basis Function Kernel model on the Electrical Grid Stability Simulated data set. The ability to make accurate predictions will be a key factor in implementing decentralized electrical grids while maintaining the required reliability. This analysis shows that machine learning algorithms can be implemented within energy management systems to make operators aware of disrupting events, such as surges in either supply or demand.

\pagebreak

**Figure 5**

*Illustration of the confusion matrix output*


```{r, echo=FALSE}
##########################################################
# Model Results
#############################################################
# cite https://stackoverflow.com/questions/23891140/r-how-to-visualize-confusion-matrix-using-the-caret-package
# this function divides the correct predictions by total number of predictions
# that tell us how accurate the model is.
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
# Plotting results
draw_confusion_matrix <- function(cm) {
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('Confusion Matrix', cex.main=1.4)
  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Class1', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Class2', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Class1', cex=1.2, srt=90)
  text(140, 335, 'Class2', cex=1.2, srt=90)
  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')
  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "Details", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.4, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.2)
  text(70, 35, names(cm$overall[2]), cex=1.4, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.2)
}  
draw_confusion_matrix(cm)
```

Future work includes further model testing on large data sets which represent readings from actual grids. The four-node star electrical grid presented here represents a very small load and supply sample. The density plot of the stability attribute showed relatively small variances across the 11 different predictors. Future studies should include data sets with disruptions from real world events in order to build confidence in implementing a reliable decentralized electrical grid.

\pagebreak

\begin{center}
\textbf{References}
\end{center}

Arzamasov, V. (2018). Electrical Grid Stability Simulated Data data set. Retrieved from  https://archive.ics.uci.edu/ml/datasets/Electrical+Grid+Stability+Simulated+Data+

Arzamasov, V., Böhm, K., & Jochem, P. (2018). Address at IEEE International  Conference. Communications, Control, and Computing Technologies for Smart Grids (SmartGridConn), Section V-A. Towards Concise Models of Grid Stability. Retrieved from https://dbis.ipd.kit.edu/download/DSGC_simulations.pdf

Dua, D. & Graff, C. (2019). UCI Machine Learning Repository. School of Information and Computer Science, University of California.

Kuhn, M. (2019). The caret Package. Retrieved from http://topepo.github.io/caret/index.html

Overleaf. (2020). Help documentation. Retrieved from https://www.overleaf.com/learn

Stack Overflow. (2017). R How to Visualize Confusion Matrix Using the Caret Package. Edited by Cybernetic. Retrieved from https://stackoverflow.com/questions/23891140/r-how-to-visualize-confusion-matrix-using-the-caret-package

The Comprehensive R Archive Network. (n.d.). A Short Introduction to the caret Package. Retrieved from https://cran.r-project.org/web/packages/caret/vignettes/caret.html

The Comprehensive R Archive Network. (n.d.). The YAML Fieldguide. Retrieved from https://cran.r-project.org/web/packages/ymlthis/vignettes/yaml-fieldguide.html
